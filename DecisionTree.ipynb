{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-59. -53. -51. ... -79. -87.   4.]\n",
      " [-66. -53. -59. ... -81. -79.   1.]\n",
      " [-41. -57. -63. ... -66. -65.   2.]\n",
      " ...\n",
      " [-57. -54. -56. ... -79. -82.   1.]\n",
      " [-56. -52. -50. ... -85. -88.   3.]\n",
      " [-46. -54. -47. ... -80. -73.   3.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_data(data):\n",
    "    \"\"\" This function splits a dataset into instances (x) and labels (y)\n",
    "    \n",
    "    Args: \n",
    "        data (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: returns tuple of (x, y) each being a numpy array\n",
    "            - x : the data instances with shape (N,7)\n",
    "            - y : the corresponsing labels with shape (N, ) \n",
    "        \n",
    "    \"\"\"\n",
    "    x = data[:,:7]\n",
    "    y = data[:,7:].reshape(data.shape[0], )\n",
    "    return (x, y)\n",
    "\n",
    "# load dataset from local machine \n",
    "clean_data = np.loadtxt(\"wifi_db/clean_dataset.txt\", dtype=int)\n",
    "noisy_data = np.loadtxt(\"wifi_db/noisy_dataset.txt\", dtype=float)\n",
    "a = noisy_data[:,-1:].astype(int)\n",
    "print(noisy_data)\n",
    "\n",
    "\n",
    "\n",
    "#clean_x, clean_y  = split_data(clean_data)\n",
    "#noisy_x, noisy_y = split_data(noisy_data)\n",
    "\n",
    "# verify the data and labels have the correct shape\n",
    "#print(clean_x.shape, clean_y.shape)\n",
    "#print(noisy_x.shape, noisy_y.shape)\n",
    "\n",
    "#a = np.unique(clean_data[:,-1:])\n",
    "#print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Creating Decision Trees #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import argmax\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \"\"\" Class representing a node data structure\n",
    "\n",
    "    Attributes:\n",
    "        left (Node): Object reference to this node's left child.\n",
    "        right (Node): Object reference to this node's right child.\n",
    "        attribute (Node): The attribute index to be tested on (0-7).\n",
    "        value (float): \n",
    "                Decision Node: The value at the indexed attribute, to be compared with the split point\n",
    "                Leaf Node:     The label of any feature vector, where the decision tree path leads to this leaf node. \n",
    "        leaf (Boolean): Specifies whether the node is a leaf node (True) or not (False) \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, left=None, right=None, attribute=None, value=None, leaf=None):\n",
    "        \"\"\" Constructor \n",
    "        Args:\n",
    "            left (Node): Object reference to this node's left child.\n",
    "            right (Node): Object reference to this node's right child.\n",
    "            attribute (Node): The attribute index to be tested on (0-7).\n",
    "            value (float): \n",
    "                Decision Node: The value at the indexed attribute, to be compared with the split point\n",
    "                Leaf Node:     The label of any feature vector, where the decision tree path leads to this leaf node. \n",
    "            leaf (Boolean): Specifies whether the node is a leaf node (True) or not (False)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        self.leaf = leaf\n",
    "    \n",
    "    # Getters -------------------------------------------------------------------------------\n",
    "    #@property\n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "    \n",
    "    #@property\n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "    \n",
    "    #@property\n",
    "    def get_attribute(self):\n",
    "        return self.attribute\n",
    "    \n",
    "    #@property\n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "    \n",
    "    #@property\n",
    "    def get_leaf(self):\n",
    "        return self.leaf\n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "    # Setters -------------------------------------------------------------------------------\n",
    "    #@property\n",
    "    def set_left(self,left):\n",
    "        self.left = left\n",
    "    \n",
    "    #@property\n",
    "    def set_right(self,right):\n",
    "        self.right = right\n",
    "    \n",
    "    #@property\n",
    "    def set_attribute(self,attribute):\n",
    "        self.attribute = attribute\n",
    "    \n",
    "    #@property\n",
    "    def set_value(self,value):\n",
    "        self.value = value\n",
    "    \n",
    "    #@property\n",
    "    def set_leaf(self,leaf):\n",
    "        self.leaf = leaf\n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"  Class schema for Decision Tree implementation\n",
    "\n",
    "     Attributes:\n",
    "        unique_labels (array): Array of unique labels in the datatset \n",
    "        root (Node): The root node of the tree (which contains an object reference to its children) \n",
    "        depth (int): Maximum path length from root to a leaf\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\" Constructor \n",
    "\n",
    "            Args:\n",
    "                dataset (numpy.ndarray): The datatset that is to be fit with a decision tree, with shape (N,8)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.unique_labels = self.get_labels(dataset)\n",
    "        self.root, self.depth = self.decision_tree_learning(dataset, 0)\n",
    "        \n",
    "    \n",
    "    # @property\n",
    "    def get_labels(self, dataset):\n",
    "        \"\"\" Creates a list of the unique labels, and assigns to the unique_labels attribute\n",
    "\n",
    "            Args:\n",
    "                dataset (numpy.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return np.unique(dataset[:,-1:])\n",
    "    \n",
    "        \n",
    "\n",
    "    # @property\n",
    "    def decision_tree_learning(self, dataset, depth):\n",
    "        \"\"\" This function fits a binary decision tree to dataset, using recursion\n",
    "        \n",
    "        Args: \n",
    "            dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "            depth (int): Maximum path length from root to a leaf. Always initialised to 0 \n",
    "        \n",
    "        Returns:\n",
    "            root (Node): The root node of the tree (which contains an object reference to its children)\n",
    "            depth (int): Maximum path length from root to a leaf\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Base Case: All dataset samples have the same label. \n",
    "        if self.have_same_label(dataset):\n",
    "            value = self.shared_label(dataset) # Create leaf node, with value = shared label\n",
    "            return self.create_leaf_node(value), depth\n",
    "        \n",
    "        # Stated assumption that there are no inconsistent data points i.e. identical feature vectors having different classes, thus no need for second base case.\n",
    "\n",
    "\n",
    "        # 2. Inductive Case\n",
    "        split_attribute, split_value = self.FIND_SPLIT(dataset)\n",
    "        \n",
    "        # instantiate node\n",
    "        new_node = Node()    \n",
    "        new_node.set_value(split_value)\n",
    "        new_node.set_attribute(split_attribute)\n",
    "\n",
    "        left_dataset, right_dataset = self.split_dataset(dataset, split_attribute, split_value)\n",
    "\n",
    "        # recursive step (depth-first fashion)\n",
    "        l_branch, l_depth = self.decision_tree_learning(left_dataset, depth+1)\n",
    "        r_branch, r_depth = self.decision_tree_learning(right_dataset, depth+1)\n",
    "\n",
    "        new_node.set_left(l_branch)\n",
    "        new_node.set_right(r_branch)\n",
    "        depth = max(l_depth, r_depth)\n",
    "\n",
    "        return new_node, depth\n",
    "\n",
    "    \n",
    "    def shared_label(self, dataset):\n",
    "        \"\"\" Returns the label shared by every sample in dataset \n",
    "            \n",
    "            *Function is called iff have_same_label() returns True\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                label (float): The label shared by all samples in dataset\n",
    "        \"\"\"\n",
    "\n",
    "        label = dataset[0,-1]\n",
    "        return label\n",
    "\n",
    "\n",
    "    def have_same_label(self, dataset):\n",
    "        \"\"\" Checks if all samples in dataset share the same label.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                shared (Boolean): True  = all samples in dataset share the same label\n",
    "                                  False = variety of labels **(meaning it can be split further)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        labels = dataset[:, -1:]\n",
    "        shared = np.all(labels == labels[0])\n",
    "        return shared\n",
    "\n",
    "\n",
    "    def create_leaf_node(self, value):\n",
    "        \"\"\" Creates a leaf node with `value` set as the label\n",
    "\n",
    "            Args: \n",
    "                value (float): The label that is to be assigned to the node's `value` attribute \n",
    "\n",
    "            Returns:\n",
    "                node (Node): A leaf node that is to be appended to the node path\n",
    "        \"\"\"\n",
    "        \n",
    "        node = Node()\n",
    "        node.set_leaf(True)\n",
    "        node.set_value(value)\n",
    "        return node\n",
    "\n",
    "\n",
    "    def FIND_SPLIT(self, dataset):\n",
    "        \"\"\" Chooses the attribute and the value that results in the highest information gain\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                Tuple:\n",
    "                    attribute (int): The index of the optimal attribute to be split at\n",
    "                    value: (float) :  The decision boundary of the attribute, which will split the incoming data\n",
    "        \"\"\"\n",
    "\n",
    "        print(self.unique_labels)\n",
    "\n",
    "        max_info_gain_per_column = []\n",
    "\n",
    "        # -1 so we don't loop over labels\n",
    "        for col in range(dataset.shape[1] - 1): \n",
    "            sorted_by_col = dataset[dataset[:, col].argsort()] # ascending order\n",
    "\n",
    "            ig_values_per_split_value_per_column = []\n",
    "\n",
    "            # loop over rows of specific attribute (column) being evaluated\n",
    "            for row in range(sorted_by_col.shape[0] - 1): \n",
    "                \"\"\"  \n",
    "                we evaluate every possible splitting value at the current attribute, perform the split, to then\n",
    "                determine the information gain of every possible split. The overall information gain for each \n",
    "                attribute is the maximum split value point.\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "                split_value = (sorted_by_col[row,col] + sorted_by_col[row+1, col]) / 2\n",
    "                left_branch, right_branch = self.split_dataset(dataset, col, split_value)\n",
    "                info_gain = self.information_gain(dataset, left_branch, right_branch)\n",
    "                ig_values_per_split_value_per_column.append(info_gain)\n",
    "                \n",
    "            max_info_gain_per_column.append(max(ig_values_per_split_value_per_column))\n",
    "\n",
    "        # we now have array of 7 items, containing the max information gain values of each attribute\n",
    "        attribute = np.argmax(max_info_gain_per_column)\n",
    "        value = np.max(max_info_gain_per_column)\n",
    "        return (attribute, value)\n",
    "\n",
    "\n",
    "    def entropy(self,dataset):\n",
    "        \"\"\" Returns the entropy of a given dataset\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                entropy (float): The calculated entropy of the dataset \n",
    "        \"\"\"\n",
    "\n",
    "        return - sum([self.p_label(dataset,label) * np.log2(self.p_label(dataset,label)) for label in self.unique_labels] )\n",
    "\n",
    "    def p_label(self, dataset, label):\n",
    "        \"\"\" Returns the proportion of dataset, that contains label values equal to label parameter\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "                label (int): The label we are evaluating the proportion of\n",
    "\n",
    "            Returns:\n",
    "                entropy (float): The calculated entropy of the dataset \n",
    "\n",
    "        \"\"\"\n",
    "        return sum([dataset[:,-1:] == int(label) for row in dataset]) / len(dataset)\n",
    "\n",
    "    def information_gain(self, dataset, left, right):\n",
    "        \"\"\" Returns the information gain from a given split of the dataset\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "                left (np.ndarray): The left split of dataset\n",
    "                right (np.ndarray): The right split of dataset\n",
    "\n",
    "            Returns:\n",
    "                information gain (float): The calculated information gain\n",
    "\n",
    "        \"\"\"\n",
    "        return self.entropy(dataset) - self.remainder(left,right)\n",
    "\n",
    "    def remainder(self, left, right):\n",
    "        \"\"\" Returns the entropy remaining after a given split. The information gain function subtracts this value \n",
    "            from the overall entropy, in order to calculate the information gained. \n",
    "\n",
    "            Args: \n",
    "                left (np.ndarray): The left split of the dataset\n",
    "                right (np.ndarray): The right split of the dataset\n",
    "\n",
    "            Returns:\n",
    "                remainder (float): The calculated entropy remainder\n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        # components\n",
    "        size_left = left.shape[0]\n",
    "        size_right = right.shape[0]\n",
    "        entropy_left = self.entropy(left)\n",
    "        entropy_right = self.entropy(right)\n",
    "\n",
    "        return ((size_left / size_left + size_right) * entropy_left) + ((size_right / size_left + size_right)* entropy_right)\n",
    "\n",
    "    def split_dataset(self, dataset, attribute, split_value):\n",
    "        \"\"\" Splits the dataset into left + right datasets, from a given split value on a specific attribute.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "                attribute (int): The column index in dataset, of the specific attribute that will be split over.\n",
    "                split_value (float): The value that each row of data will compared with. \n",
    "                                     When the value of the row indexed at attribute <= split value, that row is moved into the left branch.\n",
    "                                     When the value of the row indexed at attribute > split value, that row is moved into the right branch\n",
    "            Returns:\n",
    "                left (np.ndarray): The left split of the dataset\n",
    "                right (np.ndarray): The right split of the dataset\n",
    "                \n",
    "        \"\"\"\n",
    "        # implementational design to go left when equal to split_value\n",
    "        left = dataset[((dataset[:,attribute] <= split_value))] # https://stackoverflow.com/questions/47885848/filter-a-2d-numpy-array\n",
    "        right = dataset[((dataset[:,attribute] > split_value))] \n",
    "        return left, right\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3454697/3205624247.py:262: RuntimeWarning: divide by zero encountered in log2\n",
      "  return - sum([self.p_label(dataset,label) * np.log2(self.p_label(dataset,label)) for label in self.unique_labels] )\n",
      "/tmp/ipykernel_3454697/3205624247.py:262: RuntimeWarning: invalid value encountered in multiply\n",
      "  return - sum([self.p_label(dataset,label) * np.log2(self.p_label(dataset,label)) for label in self.unique_labels] )\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2000,1) (1999,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[39m=\u001b[39m DecisionTree(clean_data)\n",
      "Cell \u001b[0;32mIn [52], line 100\u001b[0m, in \u001b[0;36mDecisionTree.__init__\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39m\"\"\" Constructor \u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m        dataset (numpy.ndarray): The datatset that is to be fit with a decision tree, with shape (N,8)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[1;32m     97\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munique_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_labels(dataset)\n\u001b[0;32m--> 100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_tree_learning(dataset, \u001b[39m0\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [52], line 139\u001b[0m, in \u001b[0;36mDecisionTree.decision_tree_learning\u001b[0;34m(self, dataset, depth)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_leaf_node(value), depth\n\u001b[1;32m    135\u001b[0m \u001b[39m# Stated assumption that there are no inconsistent data points i.e. identical feature vectors having different classes, thus no need for second base case.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \u001b[39m# 2. Inductive Case\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m split_attribute, split_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mFIND_SPLIT(dataset)\n\u001b[1;32m    141\u001b[0m \u001b[39m# instantiate node\u001b[39;00m\n\u001b[1;32m    142\u001b[0m new_node \u001b[39m=\u001b[39m Node()    \n",
      "Cell \u001b[0;32mIn [52], line 241\u001b[0m, in \u001b[0;36mDecisionTree.FIND_SPLIT\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    239\u001b[0m     split_value \u001b[39m=\u001b[39m (sorted_by_col[row,col] \u001b[39m+\u001b[39m sorted_by_col[row\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, col]) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    240\u001b[0m     left_branch, right_branch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_dataset(dataset, col, split_value)\n\u001b[0;32m--> 241\u001b[0m     info_gain \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minformation_gain(dataset, left_branch, right_branch)\n\u001b[1;32m    242\u001b[0m     ig_values_per_split_value_per_column\u001b[39m.\u001b[39mappend(info_gain)\n\u001b[1;32m    244\u001b[0m max_info_gain_per_column\u001b[39m.\u001b[39mappend(\u001b[39mmax\u001b[39m(ig_values_per_split_value_per_column))\n",
      "Cell \u001b[0;32mIn [52], line 289\u001b[0m, in \u001b[0;36mDecisionTree.information_gain\u001b[0;34m(self, dataset, left, right)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minformation_gain\u001b[39m(\u001b[39mself\u001b[39m, dataset, left, right):\n\u001b[1;32m    278\u001b[0m     \u001b[39m\"\"\" Returns the information gain from a given split of the dataset\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \n\u001b[1;32m    280\u001b[0m \u001b[39m        Args: \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \n\u001b[1;32m    288\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentropy(dataset) \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mremainder(left,right)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2000,1) (1999,1) "
     ]
    }
   ],
   "source": [
    "a = DecisionTree(clean_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
