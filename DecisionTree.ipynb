{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import argmax\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_dataset(data, test_proportion, random_generator=default_rng()):\n",
    "    \"\"\" Split dataset into training and test sets, according to the given \n",
    "        test set proportion.\n",
    "    \n",
    "    Args:\n",
    "        *******\n",
    "        test_proprotion (float): the desired proportion of test examples \n",
    "                                 (0.0-1.0)\n",
    "        random_generator (np.random.Generator): A random generator\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple of (x_train, x_test, y_train, y_test) \n",
    "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
    "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
    "    \"\"\"\n",
    "\n",
    "    shuffled_indices = random_generator.permutation(len(data))\n",
    "    n_test = round(len(data) * test_proportion)\n",
    "    n_train = len(data) - n_test\n",
    "\n",
    "    training_data = data[shuffled_indices[:n_train]]\n",
    "    test_data = data[shuffled_indices[n_train:]]\n",
    "\n",
    "    return (training_data, test_data)\n",
    "\n",
    "\n",
    "# load dataset from local machine \n",
    "clean_data = np.loadtxt(\"wifi_db/clean_dataset.txt\", dtype=int)\n",
    "noisy_data = np.loadtxt(\"wifi_db/noisy_dataset.txt\", dtype=float)\n",
    "\n",
    "seed = 60012\n",
    "rg = default_rng(seed)\n",
    "\n",
    "clean_data_train, clean_data_test = split_dataset(clean_data, test_proportion=0.2,random_generator=rg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Creating Decision Trees #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    \"\"\" Class representing a node data structure\n",
    "\n",
    "    Attributes:\n",
    "        left (Node): Object reference to this node's left child.\n",
    "        right (Node): Object reference to this node's right child.\n",
    "        attribute (Node): The attribute index to be tested on (0-7).\n",
    "        value (float): \n",
    "                Decision Node: The value at the indexed attribute, to be compared with the split point\n",
    "                Leaf Node:     The label of any feature vector, where the decision tree path leads to this leaf node. \n",
    "        leaf (Boolean): Specifies whether the node is a leaf node (True) or not (False) \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, left=None, right=None, attribute=None, value=None, leaf=None):\n",
    "        \"\"\" Constructor \n",
    "        Args:\n",
    "            left (Node): Object reference to this node's left child.\n",
    "            right (Node): Object reference to this node's right child.\n",
    "            attribute (Node): The attribute index to be tested on (0-7).\n",
    "            value (float): \n",
    "                Decision Node: The value at the indexed attribute, to be compared with the split point\n",
    "                Leaf Node:     The label of any feature vector, where the decision tree path leads to this leaf node. \n",
    "            leaf (Boolean): Specifies whether the node is a leaf node (True) or not (False)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        self.leaf = leaf\n",
    "    \n",
    "    # Getters -------------------------------------------------------------------------------\n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "    \n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "    \n",
    "    def get_attribute(self):\n",
    "        return self.attribute\n",
    "    \n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.leaf\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "    # Setters -------------------------------------------------------------------------------\n",
    "    def set_left(self,left):\n",
    "        self.left = left\n",
    "    \n",
    "    def set_right(self,right):\n",
    "        self.right = right\n",
    "    \n",
    "    def set_attribute(self,attribute):\n",
    "        self.attribute = attribute\n",
    "    \n",
    "    def set_value(self,value):\n",
    "        self.value = value\n",
    "    \n",
    "    def set_leaf(self,leaf):\n",
    "        self.leaf = leaf\n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"  Class schema for Decision Tree implementation\n",
    "\n",
    "     Attributes:\n",
    "        unique_labels (array): Array of unique labels in the datatset \n",
    "        root (Node): The root node of the tree (which contains an object reference to its children) \n",
    "        depth (int): Maximum path length from root to a leaf\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\" Constructor \n",
    "\n",
    "            Args:\n",
    "                dataset (numpy.ndarray): The datatset that is to be fit with a decision tree, with shape (N,8)\n",
    "        \"\"\"\n",
    "\n",
    "        self.unique_labels = self.get_labels(dataset)\n",
    "        self.root, self.depth = self.decision_tree_learning(dataset, 0)\n",
    "        \n",
    "    \n",
    "    # @property\n",
    "    def get_labels(self, dataset):\n",
    "        \"\"\" Creates a list of the unique labels, and assigns to the unique_labels attribute\n",
    "\n",
    "            Args:\n",
    "                dataset (numpy.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "        \"\"\"\n",
    "\n",
    "        return np.unique(dataset[:,-1:])\n",
    "    \n",
    "    \n",
    "\n",
    "    def predict(self, samples):\n",
    "        return np.asarray([self.predict_single(self.root, sample) for sample in samples])\n",
    "\n",
    "    def predict_single(self, node, sample):\n",
    "\n",
    "        if node.is_leaf():\n",
    "            return node.get_value()\n",
    "\n",
    "        # Recursive case. Find the attribute we are splitting on and get the value\n",
    "        node_attribute = node.get_attribute()\n",
    "        node_value = node.get_value()\n",
    "        sample_value = sample[node_attribute]\n",
    "        \n",
    "        # compare\n",
    "        if sample_value <= node_value:\n",
    "            return self.predict_single(node.get_left(), sample)\n",
    "        else:\n",
    "            return self.predict_single(node.get_right(), sample)\n",
    "\n",
    "\n",
    "    # @property\n",
    "    def decision_tree_learning(self, dataset, depth):\n",
    "        \"\"\" This function fits a binary decision tree to dataset, using recursion\n",
    "        \n",
    "        Args: \n",
    "            dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "            depth (int): Maximum path length from root to a leaf. Always initialised to 0 \n",
    "        \n",
    "        Returns:\n",
    "            root (Node): The root node of the tree (which contains an object reference to its children)\n",
    "            depth (int): Maximum path length from root to a leaf\n",
    "        \"\"\"\n",
    "        # 1. Base Case: All dataset samples have the same label. \n",
    "        if self.have_same_label(dataset):\n",
    "            value = self.shared_label(dataset) # Create leaf node, with value = shared label\n",
    "            return self.create_leaf_node(value), depth\n",
    "        \n",
    "        # Stated assumption that there are no inconsistent data points i.e. identical feature vectors having different classes, thus no need for second base case.\n",
    "\n",
    "\n",
    "        # 2. Inductive Case\n",
    "        split_attribute, split_value = self.FIND_SPLIT(dataset)\n",
    "        # print(\"Found split:  {} {}\".format(split_attribute, split_value))\n",
    "        \n",
    "        # instantiate node\n",
    "        new_node = Node()    \n",
    "        new_node.set_value(split_value)\n",
    "        new_node.set_attribute(split_attribute)\n",
    "\n",
    "        left_dataset, right_dataset = self.split_dataset(dataset, split_attribute, split_value)\n",
    "\n",
    "        # recursive step (depth-first fashion)\n",
    "        l_branch, l_depth = self.decision_tree_learning(left_dataset, depth+1)\n",
    "        r_branch, r_depth = self.decision_tree_learning(right_dataset, depth+1)\n",
    "\n",
    "        new_node.set_left(l_branch)\n",
    "        new_node.set_right(r_branch)\n",
    "        depth = max(l_depth, r_depth)\n",
    "\n",
    "        return new_node, depth\n",
    "\n",
    "    \n",
    "    def shared_label(self, dataset):\n",
    "        \"\"\" Returns the label shared by every sample in dataset \n",
    "            \n",
    "            *Function is called iff have_same_label() returns True\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                label (float): The label shared by all samples in dataset\n",
    "        \"\"\"\n",
    "\n",
    "        label = dataset[0,-1]\n",
    "        return label\n",
    "\n",
    "\n",
    "    def have_same_label(self, dataset):\n",
    "        \"\"\" Checks if all samples in dataset share the same label.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                shared (Boolean): True  = all samples in dataset share the same label\n",
    "                                  False = variety of labels **(meaning it can be split further)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        labels = dataset[:, -1:]\n",
    "        shared = np.all(labels == labels[0])\n",
    "        return shared\n",
    "\n",
    "\n",
    "    def create_leaf_node(self, value):\n",
    "        \"\"\" Creates a leaf node with `value` set as the label\n",
    "\n",
    "            Args: \n",
    "                value (float): The label that is to be assigned to the node's `value` attribute \n",
    "\n",
    "            Returns:\n",
    "                node (Node): A leaf node that is to be appended to the node path\n",
    "        \"\"\"\n",
    "        \n",
    "        node = Node()\n",
    "        node.set_leaf(True)\n",
    "        node.set_value(value)\n",
    "        return node\n",
    "\n",
    "\n",
    "    def FIND_SPLIT(self, dataset):\n",
    "        # print(\"Dataset size: {}\".format(len(dataset)))\n",
    "        \"\"\" Chooses the attribute and the value that results in the highest information gain\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                Tuple:\n",
    "                    attribute (int): The index of the optimal attribute to be split at\n",
    "                    value: (float) :  The decision boundary of the attribute, which will split the incoming data\n",
    "        \"\"\"\n",
    "\n",
    "        max_info_gain = (0, 0, 0) # (info_gain, attribute, split_value)\n",
    "\n",
    "        # -1 so we don't loop over labels\n",
    "        for col in range(dataset.shape[1] - 1): \n",
    "            sorted_by_col = dataset[dataset[:, col].argsort()] # ascending order\n",
    "            # print(col)\n",
    "            # loop over rows of specific attribute (column) being evaluated\n",
    "            for row in range(sorted_by_col.shape[0] - 1): \n",
    "                \"\"\"  \n",
    "                we evaluate every possible splitting value at the current attribute, perform the split, to then\n",
    "                determine the information gain of every possible split. The overall information gain for each \n",
    "                attribute is the maximum split value point.\n",
    "\n",
    "                \"\"\"\n",
    "                split_value = (sorted_by_col[row,col] + sorted_by_col[row+1, col]) / 2\n",
    "                left_branch, right_branch = self.split_dataset(dataset, col, split_value)\n",
    "                info_gain = self.information_gain(dataset, left_branch, right_branch)\n",
    "                if info_gain > max_info_gain[0]:\n",
    "                    # update maximal \n",
    "                    max_info_gain = (info_gain, col, split_value)\n",
    "            # print(\"\")\n",
    "\n",
    "        # we now have array of 7 items, containing the max information gain values of each attribute\n",
    "        attribute = max_info_gain[1]\n",
    "        value = max_info_gain[2]\n",
    "        return (attribute, value)\n",
    "\n",
    "\n",
    "\n",
    "    def entropy(self,dataset):\n",
    "        \"\"\" Returns the entropy of a given dataset\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                entropy (float): The calculated entropy of the dataset \n",
    "        \"\"\"\n",
    "        sum = 0\n",
    "        for label in self.unique_labels:\n",
    "            proportion = self.p_label(dataset, label)\n",
    "            log_of_proportion = 0 if proportion == 0 else np.log2(proportion)\n",
    "            sum += (proportion * log_of_proportion)\n",
    "        return -sum\n",
    "\n",
    "    def p_label(self, dataset, label):\n",
    "        \"\"\" Returns the proportion of dataset, that contains label values equal to label parameter\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "                label (int): The label we are evaluating the proportion of\n",
    "\n",
    "            Returns:\n",
    "                entropy (float): The calculated entropy of the dataset \n",
    "\n",
    "        \"\"\"\n",
    "        return 0 if len(dataset) == 0 else sum([int(row[-1]) == int(label) for row in dataset]) / len(dataset)\n",
    "\n",
    "    def information_gain(self, dataset, left, right):\n",
    "        \"\"\" Returns the information gain from a given split of the dataset\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "                left (np.ndarray): The left split of dataset\n",
    "                right (np.ndarray): The right split of dataset\n",
    "\n",
    "            Returns:\n",
    "                information gain (float): The calculated information gain\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"Remainder is {}\".format(self.remainder(left, right)))\n",
    "        #print(\"Enropy is {}\".format(self.entropy(dataset)))\n",
    "\n",
    "        return self.entropy(dataset) - self.remainder(left,right)\n",
    "\n",
    "    def remainder(self, left, right):\n",
    "        \"\"\" Returns the entropy remaining after a given split. The information gain function subtracts this value \n",
    "            from the overall entropy, in order to calculate the information gained. \n",
    "\n",
    "            Args: \n",
    "                left (np.ndarray): The left split of the dataset\n",
    "                right (np.ndarray): The right split of the dataset\n",
    "\n",
    "            Returns:\n",
    "                remainder (float): The calculated entropy remainder\n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        # components\n",
    "        size_left = left.shape[0]\n",
    "        size_right = right.shape[0]\n",
    "        entropy_left = self.entropy(left)\n",
    "        entropy_right = self.entropy(right)\n",
    "        return ((size_left / (size_left + size_right)) * entropy_left) + ((size_right / (size_left + size_right))* entropy_right)\n",
    "\n",
    "    def split_dataset(self, dataset, attribute, split_value):\n",
    "        \"\"\" Splits the dataset into left + right datasets, from a given split value on a specific attribute.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "                attribute (int): The column index in dataset, of the specific attribute that will be split over.\n",
    "                split_value (float): The value that each row of data will compared with. \n",
    "                                     When the value of the row indexed at attribute <= split value, that row is moved into the left branch.\n",
    "                                     When the value of the row indexed at attribute > split value, that row is moved into the right branch\n",
    "            Returns:\n",
    "                left (np.ndarray): The left split of the dataset\n",
    "                right (np.ndarray): The right split of the dataset\n",
    "                \n",
    "        \"\"\"\n",
    "        # implementational design to go left when equal to split_value\n",
    "        left = dataset[((dataset[:,attribute] <= split_value))] # https://stackoverflow.com/questions/47885848/filter-a-2d-numpy-array\n",
    "        right = dataset[((dataset[:,attribute] > split_value))] \n",
    "        #left_mask = [row[attribute] <= split_value for row in dataset]\n",
    "        #right_mask = [row[attribute] > split_value for row in dataset]\n",
    "        #left = dataset[left_mask]\n",
    "        #right = dataset[right_mask]\n",
    "        return left, right\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTree(clean_data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def k_fold_split(k_folds, n_instances, random_generator=default_rng()):\n",
    "    \"\"\" Split k_instances into n mutually exclusive splits at random.\n",
    "    \n",
    "    Args:\n",
    "        n_splits (int): Number of splits\n",
    "        n_instances (int): Number of instances to split\n",
    "        random_generator (np.random.Generator): A random generator\n",
    "\n",
    "    Returns:\n",
    "        list: a list (length n_splits). Each element in the list should contain a \n",
    "            numpy array giving the indices of the instances in that split.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate a random permutation of indices from 0 to n_instances\n",
    "    shuffled_indices = random_generator.permutation(n_instances)\n",
    "\n",
    "    # split shuffled indices into almost equal sized splits\n",
    "    split_indices = np.array_split(shuffled_indices, k_folds)\n",
    "\n",
    "    return split_indices\n",
    "\n",
    "\n",
    "def train_test_k_fold(k_folds, n_instances, random_generator=default_rng()):\n",
    "    \"\"\" Generate train and test indices at each fold.\n",
    "    \n",
    "    Args:\n",
    "        k_folds (int): Number of folds\n",
    "        n_instances (int): Total number of instances\n",
    "        random_generator (np.random.Generator): A random generator\n",
    "\n",
    "    Returns:\n",
    "        list: a list of length k_folds. Each element in the list is a list (or tuple) \n",
    "            with two elements: a numpy array containing the train indices, and another \n",
    "            numpy array containing the test indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # split the dataset into k splits\n",
    "    split_indices = k_fold_split(k_folds, n_instances, random_generator)\n",
    "\n",
    "    folds = []\n",
    "    for k in range(k_folds):\n",
    "        # the selected k_fold for test\n",
    "        test_indices = split_indices[k]\n",
    "\n",
    "        # combine remaining splits as train\n",
    "        # this solution is fancy and worked for me\n",
    "        # feel free to use a more verbose solution that's more readable\n",
    "        train_indices = np.hstack(split_indices[:k] + split_indices[k+1:])\n",
    "\n",
    "        folds.append([train_indices, test_indices])\n",
    "\n",
    "    return folds\n",
    "\n",
    "\n",
    "def confusion_matrix(y_gold, y_prediction, class_labels=None):\n",
    "    \"\"\" Compute the confusion matrix.\n",
    "        \n",
    "    Args:\n",
    "        y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
    "        y_prediction (np.ndarray): the predicted labels\n",
    "        class_labels (np.ndarray): a list of unique class labels. \n",
    "                               Defaults to the union of y_gold and y_prediction.\n",
    "\n",
    "    Returns:\n",
    "        np.array : shape (C, C), where C is the number of classes. \n",
    "                   Rows are ground truth per class, columns are predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # if no class_labels are given, we obtain the set of unique class labels from\n",
    "    # the union of the ground truth annotation and the prediction\n",
    "    if not class_labels:\n",
    "        class_labels = np.unique(np.concatenate((y_gold, y_prediction)))\n",
    "\n",
    "    confusion = np.zeros((len(class_labels), len(class_labels)), dtype=np.int)\n",
    "\n",
    "    # for each correct class (row), \n",
    "    # compute how many instances are predicted for each class (columns)\n",
    "    for (i, label) in enumerate(class_labels):\n",
    "        # get predictions where the ground truth is the current class label\n",
    "        indices = (y_gold == label)\n",
    "        gold = y_gold[indices]\n",
    "        predictions = y_prediction[indices]\n",
    "\n",
    "        # quick way to get the counts per label\n",
    "        (unique_labels, counts) = np.unique(predictions, return_counts=True)\n",
    "\n",
    "        # convert the counts to a dictionary\n",
    "        frequency_dict = dict(zip(unique_labels, counts))\n",
    "\n",
    "        # fill up the confusion matrix for the current row\n",
    "        for (j, class_label) in enumerate(class_labels):\n",
    "            confusion[i, j] = frequency_dict.get(class_label, 0)\n",
    "\n",
    "    return confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary cell\n",
    "def accuracy(y_actual, y_predicted):\n",
    "    if len(y_actual) != len(y_predicted):\n",
    "        return None\n",
    "    correct = 0\n",
    "    for i in range(len(y_actual)):\n",
    "        if y_predicted[i] == y_actual[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(y_predicted)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 10\n",
    "accuracies = np.zeros((k_folds, ))\n",
    "\n",
    "for i, (train_indices, test_indices) in enumerate(train_test_k_fold(k_folds, len(clean_data), rg)):\n",
    "\n",
    "    # Train the KNN (we'll use one nearest neighbour)\n",
    "    decision_tree = DecisionTree(clean_data[train_indices])\n",
    "    predictions = decision_tree.predict(clean_data[test_indices])\n",
    "    acc = accuracy(clean_data[test_indices][:,-1], predictions)\n",
    "    accuracies[i] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96  0.965 0.98  0.965 0.965 0.98  0.975 0.97  0.985 0.99 ]\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4 5]\n",
      " [7 2 3 4 5]]\n",
      "[[ 1 10  3  4  5]\n",
      " [ 9  2  3  4  5]\n",
      " [ 7  2  3  4  5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([[1, 2, 3, 4, 5], [9, 2, 3, 4, 5], [7, 2, 3, 4, 5]])\n",
    "mask = [row[0] < 9 for row in arr]\n",
    "filtered = arr[mask]\n",
    "arr[0][1] = 10\n",
    "print(filtered)\n",
    "print(arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
