{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import argmax\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_dataset(data, test_proportion, val_proportion, random_generator=default_rng()):\n",
    "    \"\"\" Split dataset into training and test sets, according to the given \n",
    "        test set proportion.\n",
    "    \n",
    "    Args:\n",
    "        *******\n",
    "        test_proprotion (float): the desired proportion of test examples \n",
    "                                 (0.0-1.0)\n",
    "        random_generator (np.random.Generator): A random generator\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple of (x_train, x_test, y_train, y_test) \n",
    "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
    "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
    "    \"\"\"\n",
    "\n",
    "    shuffled_indices = random_generator.permutation(len(data))\n",
    "    n_test = round(len(data) * test_proportion)\n",
    "    n_val = round(len(data) * val_proportion)\n",
    "    n_train = len(data) - (n_test + n_val)\n",
    "\n",
    "    training_data = data[shuffled_indices[:n_train]]\n",
    "    test_data = data[shuffled_indices[n_train : n_train + n_test]]\n",
    "    validation_data = data[shuffled_indices[n_train + n_test : ]]\n",
    "\n",
    "    return (training_data, test_data, validation_data)\n",
    "\n",
    "\n",
    "# load dataset from local machine \n",
    "clean_data = np.loadtxt(\"wifi_db/clean_dataset.txt\", dtype=int)\n",
    "noisy_data = np.loadtxt(\"wifi_db/noisy_dataset.txt\", dtype=float)\n",
    "\n",
    "seed = 60012\n",
    "rg = default_rng(seed)\n",
    "\n",
    "clean_data_train, clean_data_test, clean_data_val = split_dataset(clean_data, test_proportion=0.1, val_proportion=0.1,random_generator=rg)\n",
    "noisy_data_train, noisy_data_test, noisy_data_val = split_dataset(noisy_data, test_proportion=0.1, val_proportion=0.1,random_generator=rg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Creating Decision Trees #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    \"\"\" Class representing a node data structure\n",
    "\n",
    "    Attributes:\n",
    "        left (Node): Object reference to this node's left child.\n",
    "        right (Node): Object reference to this node's right child.\n",
    "        attribute (Node): The attribute index to be tested on (0-7).\n",
    "        value (float): \n",
    "                Decision Node: The value at the indexed attribute, to be compared with the split point\n",
    "                Leaf Node:     The label of any feature vector, where the decision tree path leads to this leaf node. \n",
    "        leaf (Boolean): Specifies whether the node is a leaf node (True) or not (False) \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, left=None, right=None, attribute=None, value=None, leaf=None):\n",
    "        \"\"\" Constructor \n",
    "        Args:\n",
    "            left (Node): Object reference to this node's left child.\n",
    "            right (Node): Object reference to this node's right child.\n",
    "            attribute (Node): The attribute index to be tested on (0-7).\n",
    "            value (float): \n",
    "                Decision Node: The value at the indexed attribute, to be compared with the split point\n",
    "                Leaf Node:     The label of any feature vector, where the decision tree path leads to this leaf node. \n",
    "            leaf (Boolean): Specifies whether the node is a leaf node (True) or not (False)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        self.leaf = leaf\n",
    "        self.label_count = 0\n",
    "    \n",
    "    # Getters -------------------------------------------------------------------------------\n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "\n",
    "    def get_label_count(self):\n",
    "        return self.label_count\n",
    "    \n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "    \n",
    "    def get_attribute(self):\n",
    "        return self.attribute\n",
    "    \n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.leaf\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "    # Setters -------------------------------------------------------------------------------\n",
    "    def set_left(self,left):\n",
    "        self.left = left\n",
    "    \n",
    "    def set_right(self,right):\n",
    "        self.right = right\n",
    "    \n",
    "    def set_attribute(self,attribute):\n",
    "        self.attribute = attribute\n",
    "    \n",
    "    def set_value(self,value):\n",
    "        self.value = value\n",
    "    \n",
    "    def set_leaf(self,leaf):\n",
    "        self.leaf = leaf\n",
    "\n",
    "    def set_label_count(self, label_count):\n",
    "        self.label_count = label_count\n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"  Class schema for Decision Tree implementation\n",
    "\n",
    "     Attributes:\n",
    "        unique_labels (array): Array of unique labels in the datatset \n",
    "        root (Node): The root node of the tree (which contains an object reference to its children) \n",
    "        depth (int): Maximum path length from root to a leaf\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, validation):\n",
    "        \"\"\" Constructor \n",
    "\n",
    "            Args:\n",
    "                dataset (numpy.ndarray): The datatset that is to be fit with a decision tree, with shape (N,8)\n",
    "                validation(numpy.ndarray): The dataset that is to be used to evaluate pruning performance, with shape (N,8).\n",
    "        \"\"\"\n",
    "        self.validation = validation\n",
    "        self.unique_labels = self.get_labels(dataset)\n",
    "        self.root, self.depth = self.decision_tree_learning(dataset, 0)\n",
    "        \n",
    "    \n",
    "    def get_labels(self, dataset):\n",
    "        \"\"\" Creates a list of the unique labels, and assigns to the unique_labels attribute\n",
    "\n",
    "            Args:\n",
    "                dataset (numpy.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "        \"\"\"\n",
    "\n",
    "        return np.unique(dataset[:,-1:])\n",
    "    \n",
    "    \n",
    "\n",
    "    def predict(self, samples):\n",
    "        \"\"\" Traverses the already built decision tree, in order to make a classification label for a set of samples, relying on predict_single()\n",
    "\n",
    "            Args:\n",
    "                sample (numpy.ndarray): A dataset of unseen samples samples to be classified, with shape (N,8) \n",
    "\n",
    "             Returns:\n",
    "                labels (numpy.ndarray): Array containing label prediction of each sample, with shape (N,)\n",
    "        \"\"\"\n",
    "        return np.asarray([self.predict_single(self.root, sample) for sample in samples])\n",
    "\n",
    "    def predict_single(self, node, sample):\n",
    "        \"\"\" Traverses the already built decision tree, in order to make a classification label for a single data sample, using recursion.\n",
    "\n",
    "            Args:\n",
    "                node (Node): Object reference to the current node in the decision tree.\n",
    "                sample (numpy.ndarray): A single data sample (row) to be classified, with shape (1,8)\n",
    "\n",
    "             Returns:\n",
    "                label (int): The predicted label of our sample\n",
    "        \"\"\"\n",
    "\n",
    "        # Base Case\n",
    "        if node.is_leaf():\n",
    "            return node.get_value()\n",
    "\n",
    "        # Recursive case. Find the attribute we are splitting on and get the value\n",
    "        node_attribute = node.get_attribute()\n",
    "        node_value = node.get_value()\n",
    "        sample_value = sample[node_attribute]\n",
    "        \n",
    "        # compare\n",
    "        if sample_value <= node_value:\n",
    "            return self.predict_single(node.get_left(), sample)\n",
    "        else:\n",
    "            return self.predict_single(node.get_right(), sample)\n",
    "\n",
    "\n",
    "    def decision_tree_learning(self, dataset, depth):\n",
    "        \"\"\" This function fits a binary decision tree to dataset, using recursion\n",
    "        \n",
    "        Args: \n",
    "            dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "            depth (int): Maximum path length from root to a leaf. Always initialised to 0 \n",
    "        \n",
    "        Returns:\n",
    "            root (Node): The root node of the tree (which contains an object reference to its children)\n",
    "            depth (int): Maximum path length from root to a leaf\n",
    "        \"\"\"\n",
    "        # 1. Base Case: All dataset samples have the same label. \n",
    "        if self.have_same_label(dataset):  \n",
    "            return self.create_leaf_node(dataset), depth\n",
    "        \n",
    "\n",
    "        # Stated assumption that there are no inconsistent data points i.e. identical feature vectors having different classes, thus no need for second base case.\n",
    "\n",
    "\n",
    "        # 2. Inductive Case\n",
    "        split_attribute, split_value = self.FIND_SPLIT(dataset)\n",
    "        # print(\"Found split:  {} {}\".format(split_attribute, split_value))\n",
    "        \n",
    "        # instantiate node\n",
    "        new_node = Node()    \n",
    "        new_node.set_value(split_value)\n",
    "        new_node.set_attribute(split_attribute)\n",
    "         \n",
    "        left_dataset, right_dataset = self.split_dataset(dataset, split_attribute, split_value)\n",
    "\n",
    "        # recursive step (depth-first fashion)\n",
    "        l_branch, l_depth = self.decision_tree_learning(left_dataset, depth+1)\n",
    "        r_branch, r_depth = self.decision_tree_learning(right_dataset, depth+1)\n",
    "\n",
    "        new_node.set_left(l_branch)\n",
    "        new_node.set_right(r_branch)\n",
    "        depth = max(l_depth, r_depth)\n",
    "\n",
    "        return new_node, depth\n",
    "\n",
    "    \n",
    "    def shared_label(self, dataset):\n",
    "        \"\"\" Returns the label shared by every sample in dataset \n",
    "            \n",
    "            *Function is called iff have_same_label() returns True\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                label (float): The label shared by all samples in dataset\n",
    "        \"\"\"\n",
    "\n",
    "        label = dataset[0,-1]\n",
    "        return label\n",
    "\n",
    "\n",
    "    def have_same_label(self, dataset):\n",
    "        \"\"\" Checks if all samples in dataset share the same label.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                shared (Boolean): True  = all samples in dataset share the same label\n",
    "                                  False = variety of labels **(meaning it can be split further)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        labels = dataset[:, -1:]\n",
    "        shared = np.all(labels == labels[0])\n",
    "        return shared\n",
    "\n",
    "\n",
    "    def create_leaf_node(self, dataset):\n",
    "        \"\"\" Creates a leaf node with `value` set as the label\n",
    "\n",
    "            Args: \n",
    "                value (float): The label that is to be assigned to the node's `value` attribute \n",
    "\n",
    "            Returns:\n",
    "                node (Node): A leaf node that is to be appended to the node path\n",
    "        \"\"\"\n",
    "        value = self.shared_label(dataset)\n",
    "        label_count = len(dataset)\n",
    "        node = Node()\n",
    "        node.set_leaf(True)\n",
    "        node.set_value(value)\n",
    "        node.set_label_count(label_count)\n",
    "        return node\n",
    "\n",
    "\n",
    "    def FIND_SPLIT(self, dataset):\n",
    "        \"\"\" Chooses the attribute and the value that results in the highest information gain\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                Tuple:\n",
    "                    attribute (int): The index of the optimal attribute to be split at\n",
    "                    value: (float) :  The decision boundary of the attribute, which will split the incoming data\n",
    "        \"\"\"\n",
    "\n",
    "        # (info_gain, attribute, split_value)\n",
    "        max_info_gain = (0, 0, 0) \n",
    "\n",
    "        # -1 so we don't loop over labels\n",
    "        for col in range(dataset.shape[1] - 1): \n",
    "            sorted_by_col = dataset[dataset[:, col].argsort()] # ascending order\n",
    "\n",
    "            # loop over rows of specific attribute (column) being evaluated\n",
    "            for row in range(sorted_by_col.shape[0] - 1): \n",
    "\n",
    "                # Determine information gain for a trialled split value\n",
    "                split_value = (sorted_by_col[row,col] + sorted_by_col[row+1, col]) / 2\n",
    "                left_branch, right_branch = self.split_dataset(dataset, col, split_value)\n",
    "                info_gain = self.information_gain(dataset, left_branch, right_branch)\n",
    "\n",
    "                # update maximal \n",
    "                if info_gain > max_info_gain[0]:\n",
    "                    max_info_gain = (info_gain, col, split_value)\n",
    "\n",
    "\n",
    "        # select splitting attribute + splitting value\n",
    "        attribute = max_info_gain[1]\n",
    "        value = max_info_gain[2]\n",
    "        return (attribute, value)\n",
    "\n",
    "\n",
    "\n",
    "    def entropy(self,dataset):\n",
    "        \"\"\" Returns the entropy of a given dataset\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                entropy (float): The calculated entropy of the dataset \n",
    "        \"\"\"\n",
    "        sum = 0\n",
    "        for label in self.unique_labels:\n",
    "            proportion = self.p_label(dataset, label)\n",
    "            log_of_proportion = 0 if proportion == 0 else np.log2(proportion)  # edge case\n",
    "            sum += (proportion * log_of_proportion)\n",
    "\n",
    "        return -sum\n",
    "\n",
    "    def p_label(self, dataset, label):\n",
    "        \"\"\" Returns the proportion of the dataset that contains labels that are equal to a given label parameter\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "                label (int): The label we are evaluating the proportion of\n",
    "\n",
    "            Returns:\n",
    "                entropy (float): The calculated entropy of the dataset \n",
    "\n",
    "        \"\"\"\n",
    "        # edge case: empty dataset\n",
    "        return 0 if len(dataset) == 0 else sum([int(row[-1]) == int(label) for row in dataset]) / len(dataset)\n",
    "\n",
    "    def information_gain(self, dataset, left, right):\n",
    "        \"\"\" Returns the information gain from a given split of the dataset\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "                left (np.ndarray): The left split of dataset\n",
    "                right (np.ndarray): The right split of dataset\n",
    "\n",
    "            Returns:\n",
    "                information gain (float): The calculated information gain\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return self.entropy(dataset) - self.remainder(left,right)\n",
    "\n",
    "    def remainder(self, left, right):\n",
    "        \"\"\" Returns the entropy remaining after a given split. The information gain function subtracts this value \n",
    "            from the overall entropy, in order to calculate the information gained. \n",
    "\n",
    "            Args: \n",
    "                left (np.ndarray): The left split of the dataset\n",
    "                right (np.ndarray): The right split of the dataset\n",
    "\n",
    "            Returns:\n",
    "                remainder (float): The calculated entropy remainder\n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        # components\n",
    "        size_left = left.shape[0]\n",
    "        size_right = right.shape[0]\n",
    "        entropy_left = self.entropy(left)\n",
    "        entropy_right = self.entropy(right)\n",
    "\n",
    "        return ((size_left / (size_left + size_right)) * entropy_left) + ((size_right / (size_left + size_right))* entropy_right)\n",
    "\n",
    "    def split_dataset(self, dataset, attribute, split_value):\n",
    "        \"\"\" Splits the dataset into left + right datasets, from a given split value on a specific attribute.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "                attribute (int): The column index in dataset, of the specific attribute that will be split over.\n",
    "                split_value (float): The value that each row of data will compared with. \n",
    "                                     When the value of the row indexed at attribute <= split value, that row is moved into the left branch.\n",
    "                                     When the value of the row indexed at attribute > split value, that row is moved into the right branch\n",
    "            Returns:\n",
    "                left (np.ndarray): The left split of the dataset\n",
    "                right (np.ndarray): The right split of the dataset\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        # implementational design to go left when equal to split_value\n",
    "        left = dataset[((dataset[:,attribute] <= split_value))]\n",
    "        right = dataset[((dataset[:,attribute] > split_value))] \n",
    "\n",
    "        return left, right\n",
    "\n",
    "\n",
    "\n",
    "    def parents_of_leaves(self, node, parents):\n",
    "        \"\"\" Generates a list of all nodes in tree that are parents of two leaves.\n",
    "\n",
    "            Args: \n",
    "                node (Node): Object reference to the current node in the decision tree.\n",
    "                parents (array): The list of parent nodes, initialised to an empty list\n",
    "\n",
    "            The function does not return the parents list, it updates the parents list.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Base case\n",
    "        if node.is_leaf():\n",
    "            return\n",
    "\n",
    "        # 2. Base Case\n",
    "        if node.get_left().is_leaf() and node.get_right().is_leaf():\n",
    "            parents.append(node) # parent found\n",
    "            return\n",
    "        \n",
    "        # 3. Inductive Case (check children)\n",
    "        self.parents_of_leaves(node.get_left(), parents) \n",
    "        self.parents_of_leaves(node.get_right(), parents)\n",
    "        return \n",
    "\n",
    "\n",
    "    def majority(self, node):\n",
    "        \"\"\" Returns the majority classification label of a sub-tree rooted at `node`.\n",
    "\n",
    "            Args: \n",
    "                node (Node): Object reference to the current node in the decision tree.\n",
    "                votes (array): An array of votes for each unique class label\n",
    "\n",
    "            The function does not return the votes list, it updates the votes list.  \n",
    "        \"\"\"\n",
    "        left = node.get_left()\n",
    "        right = node.get_right()\n",
    "        left_label_counts = left.get_label_count()\n",
    "        right_label_counts = right.get_label_count()\n",
    "\n",
    "        if left_label_counts >= right_label_counts:\n",
    "            return left.get_value(), left\n",
    "        else:\n",
    "            return right.get_value(), right\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self, test_db, trained_tree):\n",
    "        # We are inside decision tree, so we don't explicitly use trained_tree, but include to be consistent with the specification.\n",
    "        y_predictions = self.predict(test_db)\n",
    "        y_gold = test_db[:,-1]\n",
    "\n",
    "        correct = sum([y_predictions[i] == y_gold[i] for i in range(len(test_db))])\n",
    "        accuracy = correct / len(test_db)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def prune(self):\n",
    "        \"\"\" Attempts to prune the already built decision tree (attempts because pruning may not improve performance, and will not be applied)\n",
    "\n",
    "            This function takes the  and does not return anything. Rather it modifies the current decision tree object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the error before pruning.\n",
    "        prev_error = (1 - self.evaluate(self.validation, self))\n",
    "\n",
    "        parents_of_leaves = []\n",
    "        self.parents_of_leaves(self.root, parents_of_leaves)\n",
    "        \n",
    "        for parent in parents_of_leaves:\n",
    "\n",
    "            m_label, m_node = self.majority(parent)\n",
    "\n",
    "            # Backup value (in case prune needs to be undone)\n",
    "            old_value = parent.get_value()\n",
    "\n",
    "            # Prune tree \n",
    "            parent.set_leaf(True)\n",
    "            parent.set_value(m_label)\n",
    "\n",
    "            # Test performance of pruned tree\n",
    "            new_error = (1 - self.evaluate(self.validation, self))\n",
    "\n",
    "            #print(\"New error is: {}\".format(new_error))\n",
    "            #print(\"Old error is: {}\".format(prev_error))\n",
    "            #print(\"\")\n",
    "            if new_error <= prev_error:                      #Set back to <= to\n",
    "                # success. Try pruning again.\n",
    "                parent.set_label_count(m_node.get_label_count())\n",
    "                self.prune()\n",
    "                return\n",
    "            else:\n",
    "                # revert to previous tree\n",
    "                parent.set_leaf(False)\n",
    "                parent.set_value(old_value)\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-pruning vaidation accuracy: 0.815\n"
     ]
    }
   ],
   "source": [
    "# pre prune\n",
    "decision_tree = DecisionTree(noisy_data_train, noisy_data_val)\n",
    "print(\"Pre-pruning vaidation accuracy: {}\".format(decision_tree.evaluate(noisy_data_val, decision_tree)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-pruning vaidation accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "# post prune\n",
    "decision_tree.prune()\n",
    "print(\"Post-pruning vaidation accuracy: {}\".format(decision_tree.evaluate(noisy_data_val, decision_tree)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "votes_per_label = np.zeros(5, dtype=int)\n",
    "print(votes_per_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def k_fold_split(k_folds, n_instances, random_generator=default_rng()):\n",
    "    \"\"\" Split k_instances into n mutually exclusive splits at random.\n",
    "    \n",
    "    Args:\n",
    "        n_splits (int): Number of splits\n",
    "        n_instances (int): Number of instances to split\n",
    "        random_generator (np.random.Generator): A random generator\n",
    "\n",
    "    Returns:\n",
    "        list: a list (length n_splits). Each element in the list should contain a \n",
    "            numpy array giving the indices of the instances in that split.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate a random permutation of indices from 0 to n_instances\n",
    "    shuffled_indices = random_generator.permutation(n_instances)\n",
    "\n",
    "    # split shuffled indices into almost equal sized splits\n",
    "    split_indices = np.array_split(shuffled_indices, k_folds)\n",
    "\n",
    "    return split_indices\n",
    "\n",
    "\n",
    "def train_test_k_fold(k_folds, n_instances, random_generator=default_rng()):\n",
    "    \"\"\" Generate train and test indices at each fold.\n",
    "    \n",
    "    Args:\n",
    "        k_folds (int): Number of folds\n",
    "        n_instances (int): Total number of instances\n",
    "        random_generator (np.random.Generator): A random generator\n",
    "\n",
    "    Returns:\n",
    "        list: a list of length k_folds. Each element in the list is a list (or tuple) \n",
    "            with two elements: a numpy array containing the train indices, and another \n",
    "            numpy array containing the test indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # split the dataset into k splits\n",
    "    split_indices = k_fold_split(k_folds, n_instances, random_generator)\n",
    "\n",
    "    folds = []\n",
    "    for k in range(k_folds):\n",
    "        test_indices = split_indices[k]\n",
    "        val_indices = split_indices[(k + 1) % k_folds]\n",
    "        train_indices = np.array([])\n",
    "        for i in range(k_folds):\n",
    "            if i != k and i!= (k + 1) % k_folds:\n",
    "                train_indices = np.concatenate((train_indices, split_indices[i]))\n",
    "        folds.append([np.array(train_indices, dtype=int), np.array(test_indices), np.array(val_indices)])\n",
    "\n",
    "    return folds\n",
    "\n",
    "\n",
    "def confusion_matrix(y_gold, y_prediction, class_labels=None):\n",
    "    \"\"\" Compute the confusion matrix.\n",
    "        \n",
    "    Args:\n",
    "        y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
    "        y_prediction (np.ndarray): the predicted labels\n",
    "        class_labels (np.ndarray): a list of unique class labels. \n",
    "                               Defaults to the union of y_gold and y_prediction.\n",
    "\n",
    "    Returns:\n",
    "        np.array : shape (C, C), where C is the number of classes. \n",
    "                   Rows are ground truth per class, columns are predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # if no class_labels are given, we obtain the set of unique class labels from\n",
    "    # the union of the ground truth annotation and the prediction\n",
    "    if not class_labels:\n",
    "        class_labels = np.unique(np.concatenate((y_gold, y_prediction)))\n",
    "\n",
    "    confusion = np.zeros((len(class_labels), len(class_labels)), dtype=np.int)\n",
    "\n",
    "    # for each correct class (row), \n",
    "    # compute how many instances are predicted for each class (columns)\n",
    "    for (i, label) in enumerate(class_labels):\n",
    "        # get predictions where the ground truth is the current class label\n",
    "        indices = (y_gold == label)\n",
    "        gold = y_gold[indices]\n",
    "        predictions = y_prediction[indices]\n",
    "\n",
    "        # quick way to get the counts per label\n",
    "        (unique_labels, counts) = np.unique(predictions, return_counts=True)\n",
    "\n",
    "        # convert the counts to a dictionary\n",
    "        frequency_dict = dict(zip(unique_labels, counts))\n",
    "\n",
    "        # fill up the confusion matrix for the current row\n",
    "        for (j, class_label) in enumerate(class_labels):\n",
    "            confusion[i, j] = frequency_dict.get(class_label, 0)\n",
    "\n",
    "    return confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37  6  3  1]\n",
      " [ 2 41  4  3]\n",
      " [ 3  3 51  1]\n",
      " [ 4  0  2 39]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2293941/2584229516.py:72: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  confusion = np.zeros((len(class_labels), len(class_labels)), dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTree(noisy_data_train, noisy_data_val)\n",
    "y_predictions = decision_tree.predict(noisy_data_test)\n",
    "y_gold = noisy_data_test[:,-1]\n",
    "confusion_matrix = confusion_matrix(y_gold, y_predictions)\n",
    "\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.745, 0.795, 0.795, 0.79 , 0.775, 0.795, 0.8  , 0.795, 0.815,\n",
       "        0.77 ]),\n",
       " array([0.885, 0.88 , 0.925, 0.86 , 0.9  , 0.895, 0.91 , 0.86 , 0.91 ,\n",
       "        0.825]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def k_fold_evaluation(data, k_folds):\n",
    "    accuracies_before = np.zeros((k_folds, ))\n",
    "    accuracies_after = np.zeros((k_folds, ))\n",
    "\n",
    "    for i, (train_indices, test_indices, val_indices) in enumerate(train_test_k_fold(k_folds, len(data), rg)):\n",
    "        print(i)\n",
    "\n",
    "        decision_tree = DecisionTree(data[train_indices], data[val_indices])\n",
    "        acc = decision_tree.evaluate(data[test_indices], decision_tree)\n",
    "\n",
    "        decision_tree.prune()\n",
    "        acc_prune = decision_tree.evaluate(data[test_indices], decision_tree)\n",
    "\n",
    "        accuracies_before[i] = acc\n",
    "        accuracies_after[i] = acc_prune\n",
    "        \n",
    "    return (accuracies_before, accuracies_after)\n",
    "\n",
    "k_fold_evaluation(noisy_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.825 0.775 0.81  0.805 0.82  0.83  0.84  0.82  0.845 0.77 ]\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4 5]\n",
      " [7 2 3 4 5]]\n",
      "[[ 1 10  3  4  5]\n",
      " [ 9  2  3  4  5]\n",
      " [ 7  2  3  4  5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([[1, 2, 3, 4, 5], [9, 2, 3, 4, 5], [7, 2, 3, 4, 5]])\n",
    "mask = [row[0] < 9 for row in arr]\n",
    "filtered = arr[mask]\n",
    "arr[0][1] = 10\n",
    "print(filtered)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Pruning and re-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parents_of_leaves(self, node, parents):\n",
    "\n",
    "    if node.is_leaf():\n",
    "        return\n",
    "\n",
    "    if node.get_left().is_leaf() and node.get_right().is_leaf():\n",
    "        parents.append(node)\n",
    "        return\n",
    "\n",
    "    self.parents_of_leaves(node.get_left())\n",
    "    self.parents_of_leaves(node.get_right())\n",
    "    return \n",
    "\n",
    "\n",
    "def majority(self):\n",
    "    return np.argmax(self.majority_label(self.root, np.zeros(len(self.unique_labels) + 1)))\n",
    "\n",
    "def majority_label(self, node, labels):\n",
    "\n",
    "    if node.is_leaf():\n",
    "        labels[node.get_value()]+=1\n",
    "        return\n",
    "    else:\n",
    "        self.majority_label(node.get_left(), labels)\n",
    "        self.majority_label(node.get_right(), labels)\n",
    "\n",
    "\n",
    "def prune(self):\n",
    "    parents_of_leaves = self.parents_of_leaves(self.root)\n",
    "    for parent in parents_of_leaves():\n",
    "        # Get majority label of child nodes\n",
    "        m_label = self.majority()\n",
    "        # Backup this node's old split value\n",
    "        old_value = parent.get_value()\n",
    "        # Make this node appear as a leaf and set majority label\n",
    "        parent.set_leaf(True)\n",
    "        parent.set_value(m_label)\n",
    "        # Test whether making this node a leaf improves performance\n",
    "        improved = self.improved_performance()\n",
    "        if improved:\n",
    "            self.prune()\n",
    "        else:\n",
    "            parent.set_leaf(False)\n",
    "            parent.set(old_value)\n",
    "    return\n",
    "        \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
