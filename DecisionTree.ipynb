{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import argmax\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_dataset(data, test_proportion, val_proportion, random_generator=default_rng()):\n",
    "    \"\"\" Split dataset into training, test and validation sets, according to the given \n",
    "        proportions.\n",
    "    \n",
    "    Args:\n",
    "        data (numpy.ndarray): The datatset that is to be split, with shape (N,8)\n",
    "        test_proprotion (float): the desired proportion of test examples \n",
    "                                 (0.0-1.0)\n",
    "        val_proprotion (float): the desired proportion of validation examples \n",
    "                                 (0.0-1.0)\n",
    "        random_generator (np.random.Generator): A random generator\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple of (training_data, test_data, validation_data) \n",
    "               - training_data (np.ndarray): Training instances shape (n_train, 8)\n",
    "               - test_data (np.ndarray): Test instances shape (n_test, 8)\n",
    "               - validation_data (np.ndarray): Validation instances shape (n_val, 8)\n",
    "    \"\"\"\n",
    "\n",
    "    shuffled_indices = random_generator.permutation(len(data))\n",
    "    n_test = round(len(data) * test_proportion)\n",
    "    n_val = round(len(data) * val_proportion)\n",
    "    n_train = len(data) - (n_test + n_val)\n",
    "\n",
    "    # split\n",
    "    training_data = data[shuffled_indices[:n_train]]\n",
    "    test_data = data[shuffled_indices[n_train : n_train + n_test]]\n",
    "    validation_data = data[shuffled_indices[n_train + n_test : ]]\n",
    "\n",
    "    return (training_data, test_data, validation_data)\n",
    "\n",
    "\n",
    "# load datasets from local machine \n",
    "clean_data = np.loadtxt(\"wifi_db/clean_dataset.txt\", dtype=int)\n",
    "noisy_data = np.loadtxt(\"wifi_db/noisy_dataset.txt\", dtype=float)\n",
    "\n",
    "# same seed (for testing)\n",
    "seed = 60012\n",
    "rg = default_rng(seed)\n",
    "\n",
    "clean_data_train, clean_data_test, clean_data_val = split_dataset(clean_data, test_proportion=0.1, val_proportion=0.1,random_generator=rg)\n",
    "noisy_data_train, noisy_data_test, noisy_data_val = split_dataset(noisy_data, test_proportion=0.1, val_proportion=0.1,random_generator=rg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Creating Decision Trees (and pruning) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    \"\"\" Class representing a node data structure\n",
    "\n",
    "    Attributes:\n",
    "        left (Node): Object reference to this node's left child.\n",
    "        right (Node): Object reference to this node's right child.\n",
    "        attribute (Node): The attribute index to be tested on (0-6).\n",
    "        value (float): \n",
    "                Decision Node: The value at the indexed attribute, to be compared with the split point\n",
    "                Leaf Node:     The label of any feature vector, where the decision tree path leads to this leaf node. \n",
    "\n",
    "        leaf (Boolean): Specifies whether the node is a leaf node (True) or not (False) \n",
    "        label_count (int) : The number of training examples at a leaf node (always 0 for a decision node)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, left=None, right=None, attribute=None, value=None, leaf=None):\n",
    "        \"\"\" Constructor \n",
    "        Args:\n",
    "            left (Node): Object reference to this node's left child.\n",
    "            right (Node): Object reference to this node's right child.\n",
    "            attribute (Node): The attribute index to be tested on (0-6).\n",
    "            value (float): \n",
    "                Decision Node: The value at the indexed attribute, to be compared with the split point\n",
    "                Leaf Node:     The label of any feature vector, where the decision tree path leads to this leaf node. \n",
    "\n",
    "            leaf (Boolean): Specifies whether the node is a leaf node (True) or not (False)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        self.leaf = leaf\n",
    "        self.label_count = 0\n",
    "    \n",
    "    # Getters -------------------------------------------------------------------------------\n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "\n",
    "    def get_label_count(self):\n",
    "        return self.label_count\n",
    "    \n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "    \n",
    "    def get_attribute(self):\n",
    "        return self.attribute\n",
    "    \n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.leaf\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "    # Setters -------------------------------------------------------------------------------\n",
    "    def set_left(self,left):\n",
    "        self.left = left\n",
    "    \n",
    "    def set_right(self,right):\n",
    "        self.right = right\n",
    "    \n",
    "    def set_attribute(self,attribute):\n",
    "        self.attribute = attribute\n",
    "    \n",
    "    def set_value(self,value):\n",
    "        self.value = value\n",
    "    \n",
    "    def set_leaf(self,leaf):\n",
    "        self.leaf = leaf\n",
    "\n",
    "    def set_label_count(self, label_count):\n",
    "        self.label_count = label_count\n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"  Class schema for Decision Tree implementation\n",
    "\n",
    "     Attributes:\n",
    "        unique_labels (array): Array of unique labels in the datatset \n",
    "        root (Node): The root node of the tree (which contains an object reference to its children) \n",
    "        depth (int): Maximum path length from root to a leaf\n",
    "        validation(numpy.ndarray): The dataset that is to be used to evaluate pruning performance, with shape (N,8).\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, validation):\n",
    "        \"\"\" Constructor \n",
    "\n",
    "            Args:\n",
    "                dataset (numpy.ndarray): The datatset that is to be fit with a decision tree, with shape (N,8)\n",
    "                validation(numpy.ndarray): The dataset that is to be used to evaluate pruning performance, with shape (N,8).\n",
    "        \"\"\"\n",
    "        self.validation = validation\n",
    "        self.unique_labels = self.get_labels(dataset)\n",
    "        self.root, self.depth = self.decision_tree_learning(dataset, 0)\n",
    "        \n",
    "    \n",
    "    def get_labels(self, dataset):\n",
    "        \"\"\" Creates a list of the unique labels, and assigns to the unique_labels attribute\n",
    "\n",
    "            Args:\n",
    "                dataset (numpy.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "        \"\"\"\n",
    "\n",
    "        return np.unique(dataset[:,-1:])\n",
    "    \n",
    "    \n",
    "\n",
    "    def predict(self, samples):\n",
    "        \"\"\" Traverses the already built decision tree, in order to make a classification label for a set of samples, relying on predict_single()\n",
    "\n",
    "            Args:\n",
    "                sample (numpy.ndarray): A dataset of unseen samples samples to be classified, with shape (N,8) \n",
    "\n",
    "             Returns:\n",
    "                labels (numpy.ndarray): Array containing label prediction of each sample, with shape (N,)\n",
    "        \"\"\"\n",
    "        return np.asarray([self.predict_single(self.root, sample) for sample in samples])\n",
    "\n",
    "    def predict_single(self, node, sample):\n",
    "        \"\"\" Traverses the already built decision tree, in order to make a classification label for a single data sample, using recursion.\n",
    "\n",
    "            Args:\n",
    "                node (Node): Object reference to the current node in the decision tree.\n",
    "                sample (numpy.ndarray): A single data sample (row) to be classified, with shape (1,8)\n",
    "\n",
    "             Returns:\n",
    "                label (int): The predicted label of our sample\n",
    "        \"\"\"\n",
    "\n",
    "        # Base Case\n",
    "        if node.is_leaf():\n",
    "            return node.get_value()\n",
    "\n",
    "        # Recursive case. Find the attribute we are splitting on and get the value\n",
    "        node_attribute = node.get_attribute()\n",
    "        node_value = node.get_value()\n",
    "        sample_value = sample[node_attribute]\n",
    "        \n",
    "        # compare\n",
    "        if sample_value <= node_value:\n",
    "            return self.predict_single(node.get_left(), sample)\n",
    "        else:\n",
    "            return self.predict_single(node.get_right(), sample)\n",
    "\n",
    "\n",
    "    def decision_tree_learning(self, dataset, depth):\n",
    "        \"\"\" This function fits a binary decision tree to dataset, using recursion\n",
    "        \n",
    "        Args: \n",
    "            dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "            depth (int): Maximum path length from root to a leaf. Always initialised to 0 \n",
    "        \n",
    "        Returns:\n",
    "            root (Node): The root node of the tree (which contains an object reference to its children)\n",
    "            depth (int): Maximum path length from root to a leaf\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Base Case: All dataset samples have the same label. \n",
    "        if self.have_same_label(dataset):  \n",
    "            return self.create_leaf_node(dataset), depth\n",
    "        \n",
    "\n",
    "        # Stated assumption that there are no inconsistent data points i.e. identical feature vectors having different classes, thus no need for second base case.\n",
    "\n",
    "\n",
    "        # 2. Inductive Case: Further splitting to do\n",
    "        split_attribute, split_value = self.FIND_SPLIT(dataset)\n",
    "        \n",
    "        # instantiate node\n",
    "        new_node = Node()    \n",
    "        new_node.set_value(split_value)\n",
    "        new_node.set_attribute(split_attribute)\n",
    "         \n",
    "        left_dataset, right_dataset = self.split_dataset(dataset, split_attribute, split_value)\n",
    "\n",
    "        # recursive step (depth-first fashion)\n",
    "        l_branch, l_depth = self.decision_tree_learning(left_dataset, depth+1)\n",
    "        r_branch, r_depth = self.decision_tree_learning(right_dataset, depth+1)\n",
    "\n",
    "        new_node.set_left(l_branch)\n",
    "        new_node.set_right(r_branch)\n",
    "        depth = max(l_depth, r_depth)\n",
    "\n",
    "        return new_node, depth\n",
    "\n",
    "    \n",
    "    def shared_label(self, dataset):\n",
    "        \"\"\" Returns the label shared by every sample in dataset \n",
    "            \n",
    "            *Function is called iff have_same_label() returns True\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                label (float): The label shared by all samples in dataset\n",
    "        \"\"\"\n",
    "\n",
    "        label = dataset[0,-1]\n",
    "        return label\n",
    "\n",
    "\n",
    "    def have_same_label(self, dataset):\n",
    "        \"\"\" Checks if all samples in dataset share the same label.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                shared (Boolean): True  = all samples in dataset share the same label\n",
    "                                  False = variety of labels **(meaning it can be split further)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        labels = dataset[:, -1:]\n",
    "        shared = np.all(labels == labels[0])\n",
    "        return shared\n",
    "\n",
    "\n",
    "    def create_leaf_node(self, dataset_at_node):\n",
    "        \"\"\" Creates a leaf node with `value` set as the label, and determines the number of training samples that reach this leaf node\n",
    "            Args: \n",
    "                dataset_at_node (numpy.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "            Returns:\n",
    "                node (Node): A leaf node that is to be appended to the node path\n",
    "        \"\"\"\n",
    "\n",
    "        value = self.shared_label(dataset_at_node)\n",
    "        label_count = len(dataset_at_node) \n",
    "\n",
    "        # instantiate and set values\n",
    "        node = Node()\n",
    "        node.set_leaf(True)\n",
    "        node.set_value(value)\n",
    "        node.set_label_count(label_count)\n",
    "        return node\n",
    "\n",
    "\n",
    "    def FIND_SPLIT(self, dataset):\n",
    "        \"\"\" Chooses the attribute and the value that results in the highest information gain\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                Tuple:\n",
    "                    attribute (int): The index of the optimal attribute to be split at\n",
    "                    value: (float) :  The decision boundary of the attribute, which will split the incoming data\n",
    "        \"\"\"\n",
    "\n",
    "        # (info_gain, attribute, split_value)\n",
    "        max_info_gain = (0, 0, 0) \n",
    "\n",
    "        # -1 so we don't loop over labels\n",
    "        for col in range(dataset.shape[1] - 1): \n",
    "            sorted_by_col = dataset[dataset[:, col].argsort()] # ascending order\n",
    "\n",
    "            # loop over rows of specific attribute (column) being evaluated\n",
    "            for row in range(sorted_by_col.shape[0] - 1): \n",
    "\n",
    "                # Determine information gain for a trialled split value\n",
    "                split_value = (sorted_by_col[row,col] + sorted_by_col[row+1, col]) / 2 # midpoint\n",
    "                left_branch, right_branch = self.split_dataset(dataset, col, split_value)\n",
    "                info_gain = self.information_gain(dataset, left_branch, right_branch)\n",
    "\n",
    "                # update maximal \n",
    "                if info_gain > max_info_gain[0]:\n",
    "                    max_info_gain = (info_gain, col, split_value)\n",
    "\n",
    "\n",
    "        # select splitting attribute + splitting value\n",
    "        attribute = max_info_gain[1]\n",
    "        value = max_info_gain[2]\n",
    "        return (attribute, value)\n",
    "\n",
    "\n",
    "\n",
    "    def entropy(self,dataset):\n",
    "        \"\"\" Returns the entropy of a given dataset\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "\n",
    "            Returns:\n",
    "                entropy (float): The calculated entropy of the dataset \n",
    "        \"\"\"\n",
    "        sum = 0\n",
    "        for label in self.unique_labels:\n",
    "            proportion = self.p_label(dataset, label)\n",
    "            log_of_proportion = 0 if proportion == 0 else np.log2(proportion)  # handle edge case\n",
    "            sum += (proportion * log_of_proportion)\n",
    "\n",
    "        return -sum\n",
    "\n",
    "    def p_label(self, dataset, label):\n",
    "        \"\"\" Returns the proportion of the dataset that contains labels that are equal to a given label parameter\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "                label (int): The label we are evaluating the proportion of\n",
    "\n",
    "            Returns:\n",
    "                entropy (float): The calculated entropy of the dataset \n",
    "\n",
    "        \"\"\"\n",
    "        # edge case: empty dataset\n",
    "        return 0 if len(dataset) == 0 else sum([int(row[-1]) == int(label) for row in dataset]) / len(dataset)\n",
    "\n",
    "    def information_gain(self, dataset, left, right):\n",
    "        \"\"\" Returns the information gain from a given split of the dataset\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "                left (np.ndarray): The left split of dataset\n",
    "                right (np.ndarray): The right split of dataset\n",
    "\n",
    "            Returns:\n",
    "                information gain (float): The calculated information gain\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return self.entropy(dataset) - self.remainder(left,right)\n",
    "\n",
    "    def remainder(self, left, right):\n",
    "        \"\"\" Returns the entropy remaining after a given split. The information gain function subtracts this value \n",
    "            from the overall entropy, in order to calculate the information gained. \n",
    "\n",
    "            Args: \n",
    "                left (np.ndarray): The left split of the dataset\n",
    "                right (np.ndarray): The right split of the dataset\n",
    "\n",
    "            Returns:\n",
    "                remainder (float): The calculated entropy remainder\n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        # components\n",
    "        size_left = left.shape[0]\n",
    "        size_right = right.shape[0]\n",
    "        entropy_left = self.entropy(left)\n",
    "        entropy_right = self.entropy(right)\n",
    "\n",
    "        return ((size_left / (size_left + size_right)) * entropy_left) + ((size_right / (size_left + size_right))* entropy_right)\n",
    "\n",
    "    def split_dataset(self, dataset, attribute, split_value):\n",
    "        \"\"\" Splits the dataset into left + right datasets, from a given split value on a specific attribute.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "                attribute (int): The column index in dataset, of the specific attribute that will be split over.\n",
    "                split_value (float): The value that each row of data will compared with. \n",
    "                                     When the value of the row indexed at attribute <= split value, that row is moved into the left branch.\n",
    "                                     When the value of the row indexed at attribute > split value, that row is moved into the right branch\n",
    "            Returns:\n",
    "                left (np.ndarray): The left split of the dataset\n",
    "                right (np.ndarray): The right split of the dataset\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        # implementational design to go left when equal to split_value\n",
    "        left = dataset[((dataset[:,attribute] <= split_value))]\n",
    "        right = dataset[((dataset[:,attribute] > split_value))] \n",
    "\n",
    "        return left, right\n",
    "\n",
    "\n",
    "\n",
    "    def parents_of_leaves(self, node, parents):\n",
    "        \"\"\" Generates a list of all nodes in tree that are parents of two leaves.\n",
    "\n",
    "            Args: \n",
    "                node (Node): Object reference to the current node in the decision tree.\n",
    "                parents (array): The list of parent nodes, initialised to an empty list\n",
    "\n",
    "            The function does not return the parents list, it updates the parents list.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Base case\n",
    "        if node.is_leaf():\n",
    "            return\n",
    "\n",
    "        # 2. Base Case\n",
    "        if node.get_left().is_leaf() and node.get_right().is_leaf():\n",
    "            parents.append(node) # parent found\n",
    "            return\n",
    "        \n",
    "        # 3. Inductive Case (check children)\n",
    "        self.parents_of_leaves(node.get_left(), parents) \n",
    "        self.parents_of_leaves(node.get_right(), parents)\n",
    "        return \n",
    "\n",
    "\n",
    "    def majority(self, node):\n",
    "        \"\"\" Returns the majority classification label of a sub-tree rooted at `node`.\n",
    "\n",
    "            Args: \n",
    "                node (Node): Object reference to the current node in the decision tree.\n",
    "\n",
    "            Returns:\n",
    "                value (int): The majority class label of node's children\n",
    "                node: (Node): The object reference to the child with the majority vote\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        left = node.get_left()\n",
    "        right = node.get_right()\n",
    "\n",
    "        # number of training samples\n",
    "        left_label_counts = left.get_label_count()\n",
    "        right_label_counts = right.get_label_count()\n",
    "\n",
    "        # pick majority\n",
    "        if left_label_counts >= right_label_counts:\n",
    "            return left.get_value(), left\n",
    "        else:\n",
    "            return right.get_value(), right\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self, test_db, trained_tree):\n",
    "        \"\"\" Computes the accuracy of an already trained decision tree on the validation/test data set.\n",
    "\n",
    "            Args: \n",
    "                test_db (numpy.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8).\n",
    "\n",
    "            Returns:\n",
    "                accuracy (float): The accuracy of the decision tree on the given datset.\n",
    "                                  (0.0-1.0)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # We are inside decision tree, so we don't explicitly use trained_tree, but include to be consistent with the specification.\n",
    "        y_predictions = self.predict(test_db)\n",
    "        y_gold = test_db[:,-1]\n",
    "\n",
    "        correct = sum([y_predictions[i] == y_gold[i] for i in range(len(test_db))])\n",
    "        accuracy = correct / len(test_db)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def prune(self):\n",
    "        \"\"\" Attempts to prune the already built decision tree (we say attempts because pruning may not improve performance, in which case will not be applied)\n",
    "\n",
    "            This function takes no parameters and does not return anything. Rather it modifies the current decision tree object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the error before pruning.\n",
    "        prev_error = (1 - self.evaluate(self.validation, self))\n",
    "\n",
    "        parents_of_leaves = []\n",
    "        self.parents_of_leaves(self.root, parents_of_leaves)\n",
    "        \n",
    "        for parent in parents_of_leaves:\n",
    "\n",
    "            m_label, m_node = self.majority(parent)\n",
    "\n",
    "            # Backup value (in case prune needs to be undone)\n",
    "            old_value = parent.get_value()\n",
    "\n",
    "            # Prune tree \n",
    "            parent.set_leaf(True)\n",
    "            parent.set_value(m_label)\n",
    "\n",
    "            # Test performance of pruned tree\n",
    "            new_error = (1 - self.evaluate(self.validation, self))\n",
    "\n",
    "            if new_error <= prev_error:                     \n",
    "                # success. Try pruning again.\n",
    "                parent.set_label_count(m_node.get_label_count())\n",
    "                self.prune()\n",
    "                return\n",
    "            else:\n",
    "                # revert to previous tree\n",
    "                parent.set_leaf(False)\n",
    "                parent.set_value(old_value)\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test decision tree on both Noisy and Clean datasets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-pruning vaidation accuracy: 0.98\n",
      "Post-pruning test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# pre prune\n",
    "decision_tree = DecisionTree(clean_data_train, clean_data_val)\n",
    "print(\"Pre-pruning vaidation accuracy: {}\".format(decision_tree.evaluate(clean_data_val, decision_tree)))\n",
    "\n",
    "# post prune\n",
    "decision_tree.prune()\n",
    "print(\"Post-pruning test accuracy: {}\".format(decision_tree.evaluate(clean_data_test, decision_tree)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-pruning vaidation accuracy: 0.815\n",
      "Post-pruning test accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# pre prune\n",
    "decision_tree = DecisionTree(noisy_data_train, noisy_data_val)\n",
    "print(\"Pre-pruning vaidation accuracy: {}\".format(decision_tree.evaluate(noisy_data_val, decision_tree)))\n",
    "\n",
    "# post prune\n",
    "decision_tree.prune()\n",
    "print(\"Post-pruning test accuracy: {}\".format(decision_tree.evaluate(noisy_data_test, decision_tree)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    " # All functions in this cell have been inspired and adapted from the lab solutions.\n",
    "\n",
    "def k_fold_split(k_folds, n_instances, random_generator=default_rng()):\n",
    "   \n",
    "    \"\"\" Split k_instances into n mutually exclusive splits at random.\n",
    "    \n",
    "    Args:\n",
    "        k_folds (int): Number of folds\n",
    "        n_instances (int): Number of instances to split\n",
    "        random_generator (np.random.Generator): A random generator\n",
    "\n",
    "    Returns:\n",
    "        split_indices: a list (length k_folds). Each element in the list should contain a \n",
    "                       numpy array giving the indices of the instances in that split.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate a random permutation of indices from 0 to n_instances\n",
    "    shuffled_indices = random_generator.permutation(n_instances)\n",
    "\n",
    "    # split shuffled indices into almost equal sized folds\n",
    "    split_indices = np.array_split(shuffled_indices, k_folds)\n",
    "\n",
    "    return split_indices\n",
    "\n",
    "\n",
    "def train_test_k_fold(k_folds, n_instances, random_generator=default_rng()):\n",
    "    \"\"\" Generate train, test and validation indices at each fold.\n",
    "    \n",
    "    Args:\n",
    "        k_folds (int): Number of folds\n",
    "        n_instances (int): Total number of instances\n",
    "        random_generator (np.random.Generator): A random generator\n",
    "\n",
    "    Returns:\n",
    "        folds: a list of length k_folds. Each element in the list is a \n",
    "               list with three elements: \n",
    "                - train_indices (numpy.ndarray): The randomly shuffled training indices\n",
    "                - test_indices (numpy.ndarray): The randomly shuffled test indices\n",
    "                - val_indices (numpy.ndarray): The randomly shuffled val indices\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # split the dataset into k folds\n",
    "    split_indices = k_fold_split(k_folds, n_instances, random_generator)\n",
    "\n",
    "    folds = []\n",
    "    for k in range(k_folds):\n",
    "        # single test + training fold\n",
    "        test_indices = split_indices[k]\n",
    "        val_indices = split_indices[(k + 1) % k_folds] # modulo operator prevents overindexing\n",
    "        train_indices = np.array([])\n",
    "        \n",
    "        # remaining k-folds for train_indices\n",
    "        for i in range(k_folds):\n",
    "            if i != k and i!= (k + 1) % k_folds:\n",
    "                train_indices = np.concatenate((train_indices, split_indices[i]))\n",
    "\n",
    "        folds.append([np.array(train_indices, dtype=int), np.array(test_indices), np.array(val_indices)])\n",
    "\n",
    "    return folds\n",
    "\n",
    "\n",
    "def confusion_matrix(y_gold, y_prediction, class_labels=None):\n",
    "    \"\"\" Compute the confusion matrix.\n",
    "        \n",
    "    Args:\n",
    "        y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
    "        y_prediction (np.ndarray): the predicted labels\n",
    "        class_labels (np.ndarray): a list of unique class labels. \n",
    "                               Defaults to the union of y_gold and y_prediction.\n",
    "\n",
    "    Returns:\n",
    "        confusion (np.array): The confusion matrix with shape (C, C), where C is the number of classes. \n",
    "                              Rows are ground truth per class, columns are predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # if no class_labels are given, we obtain the set of unique class labels from \n",
    "    # the union of the ground truth annotation and the prediction\n",
    "    if not class_labels:\n",
    "        class_labels = np.unique(np.concatenate((y_gold, y_prediction)))\n",
    "\n",
    "    confusion = np.zeros((len(class_labels), len(class_labels)), dtype=int)\n",
    "\n",
    "    # for each correct class (row), \n",
    "    # compute how many instances are predicted for each class (columns)\n",
    "    for (i, label) in enumerate(class_labels):\n",
    "        \n",
    "        # get predictions where the ground truth is the current class label\n",
    "        indices = (y_gold == label)\n",
    "        gold = y_gold[indices]\n",
    "        predictions = y_prediction[indices]\n",
    "\n",
    "        # quick way to get the counts per label\n",
    "        (unique_labels, counts) = np.unique(predictions, return_counts=True)\n",
    "\n",
    "        # convert the counts to a dictionary\n",
    "        frequency_dict = dict(zip(unique_labels, counts))\n",
    "\n",
    "        # fill up the confusion matrix for the current row\n",
    "        for (j, class_label) in enumerate(class_labels):\n",
    "            confusion[i, j] = frequency_dict.get(class_label, 0)\n",
    "\n",
    "    return confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def k_fold_evaluation(data, k_folds):\n",
    "    \"\"\" Performs k-fold cross validation on a provided dataset, and computes the accuracies and confusion matrices at every fold, before and after pruning the tree\n",
    "        \n",
    "    Args:\n",
    "        data (np.ndarray):  The datatset that is to be evaluated, with shape (N,8)\n",
    "        k_folds (int): The number of folds \n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            confusions_before (list): A list where the elements are the confusion matrix (numpy.ndarray) at every fold (before pruning)\n",
    "            confusions_after (list): A list where the elements are the confusion matrix (numpy.ndarray) at every fold (after pruning)\n",
    "            accuracies_before (list): A list where the elements are the accuracy (float) at every fold (before pruning)\n",
    "            accuracies_after (list): A list where the elements are the accuracy (float) at every fold (after pruning)\n",
    "    \"\"\"\n",
    "\n",
    "    # initialise as empty\n",
    "    confusions_before = []\n",
    "    confusions_after = []\n",
    "    accuracies_before = []\n",
    "    accuracies_after = []\n",
    "\n",
    "    for i, (train_indices, test_indices, val_indices) in enumerate(train_test_k_fold(k_folds, len(data), rg)):\n",
    "        print(\"fold {}\".format(i))\n",
    "\n",
    "        # compute accuracy + confusion matrix before pruning\n",
    "        decision_tree = DecisionTree(data[train_indices], data[val_indices])\n",
    "        predictions_before = decision_tree.predict(data[test_indices])\n",
    "        confusion_before = confusion_matrix(data[test_indices][:,-1], predictions_before)\n",
    "        accuracy_before = decision_tree.evaluate(data[test_indices], decision_tree)\n",
    "\n",
    "        # compute accuracy + confusion matrix after pruning\n",
    "        decision_tree.prune()\n",
    "        predictions_after = decision_tree.predict(data[test_indices])\n",
    "        confusion_after = confusion_matrix(data[test_indices][:,-1], predictions_after)\n",
    "        accuracy_after = decision_tree.evaluate(data[test_indices], decision_tree)\n",
    "        acc_prune = decision_tree.evaluate(data[test_indices], decision_tree)\n",
    "\n",
    "        # update accuracy list + confusion matrix list (before and after)\n",
    "        confusions_before.append(confusion_before)\n",
    "        accuracies_before.append(accuracy_before)\n",
    "        confusions_after.append(confusion_after)\n",
    "        accuracies_after.append(accuracy_after)\n",
    "        \n",
    "    return (confusions_before, confusions_after, accuracies_before, accuracies_after)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold-cross valiation on both Noisy and Clean datasets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 folds\n",
    "k_fold_evaluation(clean_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    }
   ],
   "source": [
    "# 10 folds\n",
    "confusions_before, confusions_after, accuracies_before, accuracies_after = k_fold_evaluation(noisy_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73, 0.77, 0.78, 0.805, 0.85, 0.74, 0.79, 0.84, 0.84, 0.81]\n",
      "[0.85, 0.865, 0.85, 0.9, 0.9, 0.89, 0.88, 0.865, 0.89, 0.875]\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_before)\n",
    "print(accuracies_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute average confusion matrix accross folds, and the F1, precision and recall metrics ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
